Loading data...1297 total sequences1167 train sequences130 validation sequencesmax train sequence length: 1933max validation sequence length: 799Running with multi-gpu. Number of devices: 4BERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via local_model_dir---max_epoch: 50early_stop: Falsebatch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-1-BERT_CRFlearning_rate:  2e-05use_ELMo:  False---Loading data...335 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-quantities-scibert-1-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-quantities-scibert-1-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-1-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________number of alignment issues with test set: 13to solve them consider increasing the maximum sequence input length of the model and retrain                  precision    recall  f1-score   support      <unitLeft>     0.9437    0.9030    0.9229       464     <unitRight>     0.2174    0.3846    0.2778        13   <valueAtomic>     0.8493    0.8830    0.8658       581     <valueBase>     0.9688    0.8857    0.9254        35    <valueLeast>     0.8224    0.6984    0.7554       126     <valueList>     0.6552    0.3585    0.4634        53     <valueMost>     0.7701    0.6907    0.7283        97    <valueRange>     1.0000    0.9143    0.9552        35all (micro avg.)     0.8645    0.8362    0.8501      1404Evaluation runtime: 26.889 seconds Loading data...335 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-quantities-scibert-2-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-quantities-scibert-2-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-2-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________number of alignment issues with test set: 13to solve them consider increasing the maximum sequence input length of the model and retrain                  precision    recall  f1-score   support      <unitLeft>     0.9178    0.8901    0.9037       464     <unitRight>     0.2400    0.4615    0.3158        13   <valueAtomic>     0.8481    0.8744    0.8610       581     <valueBase>     1.0000    0.9143    0.9552        35    <valueLeast>     0.8571    0.7143    0.7792       126     <valueList>     0.6154    0.4528    0.5217        53     <valueMost>     0.8161    0.7320    0.7717        97    <valueRange>     1.0000    0.9143    0.9552        35all (micro avg.)     0.8590    0.8376    0.8482      1404Evaluation runtime: 25.607 seconds Loading data...335 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-quantities-scibert-3-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-quantities-scibert-3-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-3-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________number of alignment issues with test set: 13to solve them consider increasing the maximum sequence input length of the model and retrain                  precision    recall  f1-score   support      <unitLeft>     0.9438    0.9052    0.9241       464     <unitRight>     0.2381    0.3846    0.2941        13   <valueAtomic>     0.8557    0.8778    0.8666       581     <valueBase>     1.0000    0.9143    0.9552        35    <valueLeast>     0.8304    0.7381    0.7815       126     <valueList>     0.6176    0.3962    0.4828        53     <valueMost>     0.7841    0.7113    0.7459        97    <valueRange>     1.0000    0.9143    0.9552        35all (micro avg.)     0.8691    0.8419    0.8553      1404Evaluation runtime: 26.238 seconds Loading data...335 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-quantities-scibert-4-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-quantities-scibert-4-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-4-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________number of alignment issues with test set: 13to solve them consider increasing the maximum sequence input length of the model and retrain                  precision    recall  f1-score   support      <unitLeft>     0.9270    0.9030    0.9148       464     <unitRight>     0.2381    0.3846    0.2941        13   <valueAtomic>     0.8591    0.8812    0.8700       581     <valueBase>     0.9688    0.8857    0.9254        35    <valueLeast>     0.8796    0.7540    0.8120       126     <valueList>     0.5116    0.4151    0.4583        53     <valueMost>     0.8068    0.7320    0.7676        97    <valueRange>     1.0000    0.9143    0.9552        35all (micro avg.)     0.8652    0.8454    0.8552      1404Evaluation runtime: 25.737 seconds Loading data...335 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-quantities-scibert-5-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-quantities-scibert-5-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 6max_sequence_length: 512model_name: grobid-quantities-scibert-5-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  14202                                                                       model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,952,634Trainable params: 109,952,634Non-trainable params: 0_________________________________________________________________number of alignment issues with test set: 13to solve them consider increasing the maximum sequence input length of the model and retrain                  precision    recall  f1-score   support      <unitLeft>     0.9244    0.8966    0.9103       464     <unitRight>     0.2500    0.3846    0.3030        13   <valueAtomic>     0.8607    0.8830    0.8717       581     <valueBase>     1.0000    0.9143    0.9552        35    <valueLeast>     0.8396    0.7063    0.7672       126     <valueList>     0.6552    0.3585    0.4634        53     <valueMost>     0.7692    0.7216    0.7447        97    <valueRange>     1.0000    0.9143    0.9552        35all (micro avg.)     0.8673    0.8376    0.8522      1404Evaluation runtime: 25.348 seconds 