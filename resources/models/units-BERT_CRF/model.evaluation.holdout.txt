Loading data...1681 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-units_1-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-units_1-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  5446                                                                        model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 20max_sequence_length: 100model_name: grobid-units_1-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to                     ================================================================================================== input_token (InputLayer)       [(None, None)]       0           []                                                                                                                                  input_attention_mask (InputLay  [(None, None)]      0           []                                er)                                                                                                                                                                                                 input_token_type (InputLayer)  [(None, None)]       0           []                                                                                                                                  tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                            thPoolingAndCrossAt               'input_attention_mask[0][0]',                                   tentions(last_hidde               'input_token_type[0][0]']                                       n_state=(None, None                                                                               , 768),                                                                                            pooler_output=(Non                                                                               e, 768),                                                                                           past_key_values=No                                                                               ne, hidden_states=N                                                                               one, attentions=Non                                                                               e, cross_attentions                                                                               =None)                                                                                                                                                               dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']                                                                                                            ==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #   ================================================================= crf (CRF)                   multiple                  5446                                                                        model (Functional)          (None, None, 768)         109938432                                                                  =================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________                  precision    recall  f1-score   support          <base>     0.7247    0.7543    0.7392      3228           <pow>     0.9548    0.5825    0.7236      1775        <prefix>     0.7597    0.9091    0.8277      1287all (micro avg.)     0.7754    0.7375    0.7560      6290Evaluation runtime: 31.649 secondsLoading data...1681 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-units_2-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-units_2-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 20max_sequence_length: 100model_name: grobid-units_2-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________                  precision    recall  f1-score   support          <base>     0.7741    0.7773    0.7757      3228           <pow>     0.7159    0.5521    0.6234      1775        <prefix>     0.7786    0.9099    0.8391      1287all (micro avg.)     0.7622    0.7409    0.7514      6290Evaluation runtime: 22.523 secondsLoading data...1681 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-units_3-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-units_3-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 20max_sequence_length: 100model_name: grobid-units_3-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________                  precision    recall  f1-score   support          <base>     0.6745    0.7615    0.7154      3228           <pow>     0.9198    0.6592    0.7680      1775        <prefix>     0.8082    0.8904    0.8473      1287all (micro avg.)     0.7537    0.7590    0.7563      6290Evaluation runtime: 22.738 secondsLoading data...1681 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-units_4-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-units_4-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 20max_sequence_length: 100model_name: grobid-units_4-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________                  precision    recall  f1-score   support          <base>     0.7545    0.7720    0.7631      3228           <pow>     0.7264    0.5639    0.6350      1775        <prefix>     0.8750    0.7887    0.8296      1287all (micro avg.)     0.7718    0.7167    0.7432      6290Evaluation runtime: 22.01 secondsLoading data...1681 evaluation sequencesBERT_CRFallenai/scibert_scivocab_cased/dir will be used, loaded via delft_modelload weights from data/models/sequenceLabelling/grobid-units_5-BERT_CRF/model_weights.hdf5loading model weights data/models/sequenceLabelling/grobid-units_5-BERT_CRF/model_weights.hdf5Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________Evaluation:---max_epoch: 50early_stop: Truepatience: 5batch_size (training): 20max_sequence_length: 100model_name: grobid-units_5-BERT_CRFlearning_rate:  0.001use_ELMo:  False---Model: "model"__________________________________________________________________________________________________ Layer (type)                   Output Shape         Param #     Connected to================================================================================================== input_token (InputLayer)       [(None, None)]       0           [] input_attention_mask (InputLay  [(None, None)]      0           [] er) input_token_type (InputLayer)  [(None, None)]       0           [] tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',                                thPoolingAndCrossAt               'input_attention_mask[0][0]',                                tentions(last_hidde               'input_token_type[0][0]']                                n_state=(None, None                                , 768),                                 pooler_output=(Non                                e, 768),                                 past_key_values=No                                ne, hidden_states=N                                one, attentions=Non                                e, cross_attentions                                =None) dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']==================================================================================================Total params: 109,938,432Trainable params: 109,938,432Non-trainable params: 0__________________________________________________________________________________________________Model: "crf_model_wrapper_for_bert"_________________________________________________________________ Layer (type)                Output Shape              Param #================================================================= crf (CRF)                   multiple                  5446 model (Functional)          (None, None, 768)         109938432=================================================================Total params: 109,943,878Trainable params: 109,943,878Non-trainable params: 0_________________________________________________________________                  precision    recall  f1-score   support          <base>     0.7537    0.7481    0.7509      3228           <pow>     0.6930    0.5099    0.5875      1775        <prefix>     0.6591    0.9044    0.7625      1287all (micro avg.)     0.7145    0.7129    0.7137      6290Evaluation runtime: 22.148 seconds(delft2) [lfoppian0@mdpfdc001 delft_tf2_transformers]$