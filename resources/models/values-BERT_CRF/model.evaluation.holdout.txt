Loading data...
943 evaluation sequences
BERT_CRF
allenai/scibert_scivocab_cased/dir will be used, loaded via delft_model
load weights from data/models/sequenceLabelling/grobid-values_1-BERT_CRF/model_weights.hdf5
loading model weights data/models/sequenceLabelling/grobid-values_1-BERT_CRF/model_weights.hdf5
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________

Evaluation:
---
max_epoch: 50
early_stop: True
patience: 5
batch_size (training): 20
max_sequence_length: 100
model_name: grobid-values_1-BERT_CRF
learning_rate:  0.001
use_ELMo:  False
---
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________
                  precision    recall  f1-score   support

         <alpha>     0.9921    0.9921    0.9921       126
          <base>     1.0000    1.0000    1.0000        13
        <number>     0.9926    0.9951    0.9938       811
           <pow>     1.0000    1.0000    1.0000        13

all (micro avg.)     0.9927    0.9948    0.9938       963

Evaluation runtime: 18.846 seconds 
Loading data...
943 evaluation sequences
BERT_CRF
allenai/scibert_scivocab_cased/dir will be used, loaded via delft_model
load weights from data/models/sequenceLabelling/grobid-values_2-BERT_CRF/model_weights.hdf5
loading model weights data/models/sequenceLabelling/grobid-values_2-BERT_CRF/model_weights.hdf5
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________

Evaluation:
---
max_epoch: 50
early_stop: True
patience: 5
batch_size (training): 20
max_sequence_length: 100
model_name: grobid-values_2-BERT_CRF
learning_rate:  0.001
use_ELMo:  False
---
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________
                  precision    recall  f1-score   support

         <alpha>     0.9921    0.9921    0.9921       126
          <base>     1.0000    1.0000    1.0000        13
        <number>     0.9951    0.9951    0.9951       811
           <pow>     1.0000    1.0000    1.0000        13

all (micro avg.)     0.9948    0.9948    0.9948       963

Evaluation runtime: 18.262 seconds 
Loading data...
943 evaluation sequences
BERT_CRF
allenai/scibert_scivocab_cased/dir will be used, loaded via delft_model
load weights from data/models/sequenceLabelling/grobid-values_3-BERT_CRF/model_weights.hdf5
loading model weights data/models/sequenceLabelling/grobid-values_3-BERT_CRF/model_weights.hdf5
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________

Evaluation:
---
max_epoch: 50
early_stop: True
patience: 5
batch_size (training): 20
max_sequence_length: 100
model_name: grobid-values_3-BERT_CRF
learning_rate:  0.001
use_ELMo:  False
---
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________
                  precision    recall  f1-score   support

         <alpha>     0.9921    1.0000    0.9960       126
          <base>     1.0000    1.0000    1.0000        13
        <number>     0.9951    0.9938    0.9944       811
           <pow>     1.0000    1.0000    1.0000        13

all (micro avg.)     0.9948    0.9948    0.9948       963

Evaluation runtime: 18.309 seconds 
Loading data...
943 evaluation sequences
BERT_CRF
allenai/scibert_scivocab_cased/dir will be used, loaded via delft_model
load weights from data/models/sequenceLabelling/grobid-values_4-BERT_CRF/model_weights.hdf5
loading model weights data/models/sequenceLabelling/grobid-values_4-BERT_CRF/model_weights.hdf5
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________

Evaluation:
---
max_epoch: 50
early_stop: True
patience: 5
batch_size (training): 20
max_sequence_length: 100
model_name: grobid-values_4-BERT_CRF
learning_rate:  0.001
use_ELMo:  False
---
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________
                  precision    recall  f1-score   support

         <alpha>     0.9921    0.9921    0.9921       126
          <base>     1.0000    1.0000    1.0000        13
        <number>     0.9938    0.9951    0.9945       811
           <pow>     1.0000    1.0000    1.0000        13

all (micro avg.)     0.9938    0.9948    0.9943       963

Evaluation runtime: 18.673 seconds 
Loading data...
943 evaluation sequences
BERT_CRF
allenai/scibert_scivocab_cased/dir will be used, loaded via delft_model
load weights from data/models/sequenceLabelling/grobid-values_5-BERT_CRF/model_weights.hdf5
loading model weights data/models/sequenceLabelling/grobid-values_5-BERT_CRF/model_weights.hdf5
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________

Evaluation:
---
max_epoch: 50
early_stop: True
patience: 5
batch_size (training): 20
max_sequence_length: 100
model_name: grobid-values_5-BERT_CRF
learning_rate:  0.001
use_ELMo:  False
---
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_token (InputLayer)       [(None, None)]       0           []                               
                                                                                                  
 input_attention_mask (InputLay  [(None, None)]      0           []                               
 er)                                                                                              
                                                                                                  
 input_token_type (InputLayer)  [(None, None)]       0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['input_token[0][0]',            
                                thPoolingAndCrossAt               'input_attention_mask[0][0]',   
                                tentions(last_hidde               'input_token_type[0][0]']       
                                n_state=(None, None                                               
                                , 768),                                                           
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dropout_37 (Dropout)           (None, None, 768)    0           ['tf_bert_model[0][0]']          
                                                                                                  
==================================================================================================
Total params: 109,938,432
Trainable params: 109,938,432
Non-trainable params: 0
__________________________________________________________________________________________________
Model: "crf_model_wrapper_for_bert"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 crf (CRF)                   multiple                  7810      
                                                                 
 model (Functional)          (None, None, 768)         109938432 
                                                                 
=================================================================
Total params: 109,946,242
Trainable params: 109,946,242
Non-trainable params: 0
_________________________________________________________________
                  precision    recall  f1-score   support

         <alpha>     0.9921    0.9921    0.9921       126
          <base>     1.0000    1.0000    1.0000        13
        <number>     0.9951    0.9938    0.9944       811
           <pow>     1.0000    1.0000    1.0000        13

all (micro avg.)     0.9948    0.9938    0.9943       963

Evaluation runtime: 19.276 seconds 
